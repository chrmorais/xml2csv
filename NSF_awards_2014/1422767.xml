<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>RI: Small: Using Prediction to Build a Compact Visual Memex Memory for Rapid Analysis and Understanding of Egocentric Video Data</AwardTitle>
    <AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2019</AwardExpirationDate>
    <AwardAmount>368696</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Directorate for Computer &amp; Information Science &amp; Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Jie Yang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This project develops new data-driven techniques for egocentric (first-person) video stream analysis that exploit the structure and redundancy in streams captured over days, months, and even years, to significantly reduce the size of these datasets without losing the most useful visual information. Simultaneously, the research team is developing parallel programming frameworks that simplify expression and acceleration of these video analysis algorithms at scale. While the focus of this research is the design of core algorithms and systems, success stands to enable the development of new classes of applications (in domains such as navigation, personal assistance, health/behavior monitoring) that use the extensive visual history of a camera to intelligently interpret continuous visual data sources and immediately respond to the observed input. A further output of this research is the collection and organization of a large egocentric video database from the life of a single individual.&lt;br/&gt;&lt;br/&gt;The core idea of this research is to identify and exploit redundancy in everyday life. While it is not tractable to maintain an easily analyzable representation of all video ever seen by a camera, it is likely possible to identify and provide future applications fast access to the most important visual information. The challenge is to determine what visual data is the most important. This work explores the use of video stream predictability as a notion of importance. Specifically, the vast visual history of the camera (e.g., life experiences captured by a head-mounted camera) is used to make predictions about what the camera will see next, and the accuracy of these predictions dictates what data is retained. (Highly predictable occurrences are judged to be less valuable to retain in the database.) In addition, this research is characterizing the structure of always-on egocentric video streams (What is the "working set" of a person's day? How much novel information is collected from day to day?), leveraging this structure to inform the design of new algorithms for video corpus analysis (data compression, accelerated retrieval), and exploring the design of specialized programming abstractions for authoring visual data understanding applications at scale.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;URL: http://graphics.cs.cmu.edu/projects/egocentricPrediction</AbstractNarration>
    <MinAmdLetterDate>08/25/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>08/25/2014</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1422767</AwardID>
    <Investigator>
      <FirstName>Kayvon</FirstName>
      <LastName>Fatahalian</LastName>
      <EmailAddress>kayvonf@cs.cmu.edu</EmailAddress>
      <StartDate>08/25/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Carnegie-Mellon University</Name>
      <CityName>PITTSBURGH</CityName>
      <ZipCode>152133815</ZipCode>
      <PhoneNumber>4122689527</PhoneNumber>
      <StreetAddress>5000 Forbes Avenue</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Pennsylvania</StateName>
      <StateCode>PA</StateCode>
    </Institution>
  </Award>
</rootTag>
