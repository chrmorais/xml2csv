<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>CAREER: Situated Recognition: Learning to understand our local visual environment</AwardTitle>
    <AwardEffectiveDate>03/01/2015</AwardEffectiveDate>
    <AwardExpirationDate>02/29/2020</AwardExpirationDate>
    <AwardAmount>509965</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Jie Yang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This project develops computer vision technologies for recognizing objects in our daily lives. For recognizing visual content around us, where cameras can record multiple images over a period of time, there is an opportunity to take advantage of context that is not available for internet images. This project pursues new representations and computational strategies exploiting this context efficiently to achieve high-quality visual recognition in our environment. Balanced against the opportunity of using this context is the challenge of making recognition work in any particular environment, in the face of clutter, occlusion, non-canonical views, and idiosyncratic appearance variation. The methods developed can be a core part of developing technology to help computer vision systems scale to recognize everything in our daily world. The research leads to automated systems for better understanding and monitoring of our daily environment, improved human-computer interaction, and encourages more research in this area.&lt;br/&gt;&lt;br/&gt;This research direction is different from the majority of work in recognition that has focused on internet images collected from the web. The biases of such web-collected may lead to models that do not generalize to a particular environment. Situated recognition allows exploiting local context, including human interaction and spoken language, to build models specific to an environment and furthermore to the parts of an environment that are important to people. The project collects multiple datasets stressing multi-view imagery and long-term observation of environments while sampling a wide variety of settings. The research team develops algorithms to parse and detect objects by exploiting context, efficient re-use, and context-dependent saliency; and uses situated natural language to drive automatic learning of visual recognition models.&lt;br/&gt;&lt;br/&gt;Project Webpage: http://acberg.com</AbstractNarration>
    <MinAmdLetterDate>03/03/2015</MinAmdLetterDate>
    <MaxAmdLetterDate>03/03/2015</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1452851</AwardID>
    <Investigator>
      <FirstName>Alexander</FirstName>
      <LastName>Berg</LastName>
      <EmailAddress>aberg@cs.unc.edu</EmailAddress>
      <StartDate>03/03/2015</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of North Carolina at Chapel Hill</Name>
      <CityName>CHAPEL HILL</CityName>
      <ZipCode>275991350</ZipCode>
      <PhoneNumber>9199663411</PhoneNumber>
      <StreetAddress>104 AIRPORT DR STE 2200</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>North Carolina</StateName>
      <StateCode>NC</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>1045</Code>
      <Text>CAREER: FACULTY EARLY CAR DEV</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramReference>
  </Award>
</rootTag>
