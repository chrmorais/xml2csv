<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Collaborative Research: Using Somatosensory Speech And Non-Speech Categories To Test The Brain's General Principles Of Perceptual Learning</AwardTitle>
    <AwardEffectiveDate>09/15/2014</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2017</AwardExpirationDate>
    <AwardAmount>271602</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>04040000</Code>
      <Directorate>
        <LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Behavioral and Cognitive Sci</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Alumit Ishai</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>The human brain displays astonishing adaptation to novel types of sensory information. An example of such adaptation is deaf-blind individuals who learned to perceive spoken language through their sense of touch, by placing a hand on the face and throat of someone producing speech. This example tells us that the somatosensory system can carry out speech perception, which is normally thought to be in the domain of hearing. Drs. Maximilian Riesenhuber of Georgetown University and Lynne E. Bernstein of George Washington University along with their multidisciplinary team will use advanced functional magnetic resonance brain imaging (fMRI) and electroencephalography (EEG) to investigate the neural mechanisms underlying the learning of artificial categories and speech categories by the somatosensory system. In their research they are using a novel transducer to present high-dimensional stimuli to the forearm of participants who are trained on artificial or speech categories. The team is addressing whether perceptual learning of artificial categories of somatosensory patterns follows principles known to govern auditory and visual category learning. For their second aim, the researchers are training participants to recognize spoken words that are transformed into patterns of vibration. The speech stimuli are designed to address questions about cross-sensory learning and the linking of speech categories across hearing and vision. Before and following training, fMRI and EEG measures are being applied to determine where and when in the brain newly learned categories are represented. This project is pushing the frontiers of knowledge about the brain's plasticity for learning novel somatosensory categories, including showing for the first time the neural bases for speech learning through the sense of touch.&lt;br/&gt;&lt;br/&gt;Understanding the general principles of sensory processing in the brain, and in particular the commonalities and differences in the underlying neural mechanisms across sensory modalities, is of great interest for practical applications such as the design of neuroprostheses for hearing and/or vision disorders. For example, patients who have auditory or visual sensory system damage may benefit from devices that substitute vibrotactile stimuli for information no longer available through their damaged sensory systems. Vibrotactile stimuli can be combined with visual or auditory stimuli to improve speech perception in noisy situations such as the cockpit of a plane. The fMRI and EEG data from this project along with detailed records kept during training of participants will be made available to the research community. The brain measures obtained before and after training will be valuable for cost-effective testing of new hypotheses about brain plasticity and learning. Research results will be broadly disseminated through publications and conference presentations. The research project will also be leveraged extensively to train the next generation of scientists, at the graduate and undergraduate level, with a particular focus on underrepresented minorities.</AbstractNarration>
    <MinAmdLetterDate>08/14/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>08/14/2014</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1439339</AwardID>
    <Investigator>
      <FirstName>Lynne</FirstName>
      <LastName>Bernstein</LastName>
      <EmailAddress>lbernste@gwu.edu</EmailAddress>
      <StartDate>08/14/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>George Washington University</Name>
      <CityName>Washington</CityName>
      <ZipCode>200522000</ZipCode>
      <PhoneNumber>2029946255</PhoneNumber>
      <StreetAddress>2121 Eye Street NW</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>District of Columbia</StateName>
      <StateCode>DC</StateCode>
    </Institution>
    <ProgramElement>
      <Code>1699</Code>
      <Text>COGNEURO</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>1699</Code>
      <Text>COGNEURO</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7298</Code>
      <Text>COLLABORATIVE RESEARCH</Text>
    </ProgramReference>
  </Award>
</rootTag>
