<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>XPS: FULL: CCA: Collaborative Research: Automatically Scalable Computation</AwardTitle>
    <AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2015</AwardExpirationDate>
    <AwardAmount>115000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05010000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Computing and Communication Foundations</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Hong Jiang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>The era of performance scaling by increasing the performance of&lt;br/&gt;individual processors is over, having been replaced by the era of&lt;br/&gt;massive parallelism via multiple cores. Amdahl's law tells us that&lt;br/&gt;our ability to parallelize computation is limited by the inherently&lt;br/&gt;sequential portion of a computation. This unfortunate combination&lt;br/&gt;of facts paints a bleak picture for the future of scalable software.&lt;br/&gt;This work explores a radical new approach to parallelism with the&lt;br/&gt;potential to bypass Amdahl's Law. The approach used involves making&lt;br/&gt;informed predictions about computation likely to happen in the&lt;br/&gt;future, proactively executing likely computations in parallel with&lt;br/&gt;the actual computation, and then "jumping forward in time" if the&lt;br/&gt;actual execution stumbles upon any of the predicted computations&lt;br/&gt;that have already been completed. This research touches many areas&lt;br/&gt;within Computer Science, i.e., architecture, compilers, machine learning,&lt;br/&gt;systems, and theory. Additionally, exploiting massively parallel&lt;br/&gt;computation will produce immediate returns in multiple scientific&lt;br/&gt;fields that rely on computation. The research here provides an&lt;br/&gt;approach to speedup on such real-world problems.&lt;br/&gt;&lt;br/&gt;The approach used in this research views computational execution&lt;br/&gt;as moving a system through the enormously high dimensional space&lt;br/&gt;represented by its registers and memory of a conventional single-threaded&lt;br/&gt;processor. It uses machine learning algorithms to observe execution&lt;br/&gt;patterns to make predictions about likely future states of the&lt;br/&gt;computation. Based on these predictions, the system launches&lt;br/&gt;potentially large numbers of speculative threads to execute from&lt;br/&gt;these likely computations, while the actual computation proceeds&lt;br/&gt;serially. At strategically chosen points, the main computation&lt;br/&gt;queries the speculative executions to determine if any of the&lt;br/&gt;completed computation is useful; if it is, the main thread uses the&lt;br/&gt;speculative computation to immediately begin execution where the&lt;br/&gt;speculative computation left off, achieving a speed-up over the&lt;br/&gt;serial execution. This approach has the potential to be infinitely&lt;br/&gt;scalable: the more cores, memory, and communication bandwidth&lt;br/&gt;available, the greater the potential for performance improvement.&lt;br/&gt;The approach also scales across programs -- if the program running&lt;br/&gt;today happens upon a state encountered by a program running yesterday,&lt;br/&gt;the program can reuse yesterday's computation.</AbstractNarration>
    <MinAmdLetterDate>08/26/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>08/26/2014</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1438983</AwardID>
    <Investigator>
      <FirstName>Ryan</FirstName>
      <LastName>Adams</LastName>
      <EmailAddress>rpa@seas.harvard.edu</EmailAddress>
      <StartDate>08/26/2014</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Margo</FirstName>
      <LastName>Seltzer</LastName>
      <EmailAddress>margo@eecs.harvard.edu</EmailAddress>
      <StartDate>08/26/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>David</FirstName>
      <LastName>Brooks</LastName>
      <EmailAddress>dbrooks@eecs.harvard.edu</EmailAddress>
      <StartDate>08/26/2014</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Harvard University</Name>
      <CityName>Cambridge</CityName>
      <ZipCode>021383846</ZipCode>
      <PhoneNumber>6174955501</PhoneNumber>
      <StreetAddress>1350 MASSACHUSETTS AVE</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Massachusetts</StateName>
      <StateCode>MA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>8283</Code>
      <Text>Exploiting Parallel&amp;Scalabilty</Text>
    </ProgramElement>
  </Award>
</rootTag>
