<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>CAREER: Simulating Nonlinear Audiovisual Dynamics for Simulated Environments and Interactive Applications</AwardTitle>
    <AwardEffectiveDate>02/01/2015</AwardEffectiveDate>
    <AwardExpirationDate>01/31/2020</AwardExpirationDate>
    <AwardAmount>120407</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Ephraim P. Glinert</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Computer simulated virtual dynamics are increasingly recognized as important tools for creating immersive simulated environments for understanding physical systems that are too costly or perilous to investigate in real experiments. Most simulation techniques, however, are still inherently silent; they encompass only the visual modality, resulting in a limited sensory depiction of phenomena which is in sharp contrast to the real world where we are constantly immersed in rich audiovisual experiences involving both scene illumination and auditory cues. Consequently, most computer animations, regardless of how efficiently they are simulated, currently rely on manual editing to add sounds as afterthoughts, and many computational design tools are still appearance-oriented, modeling only geometry or visible motion. In this project the PI's central objective is to build efficient models for simulating nonlinear audiovisual dynamics in a realistic and synchronized way. This is a highly challenging task, as the simulation models need to faithfully resolve visible and audible details across a wide range of spatial and temporal scales because multiple scales of motion are often coupled together to produce perceivable nonlinear sound effects such as pitch shift and mode locking. The PI will address these challenges in three stages. First, he will create efficient multi-scale models which adaptively exploit underlying dynamical structures. He will then systematically evaluate these models using ground-truth data, experimental statistics and user studies. Finally, building upon his new models, he will develop tools to enable interactive parameter exploration of desired audiovisual effects.&lt;br/&gt;&lt;br/&gt;This work will seek to answer two fundamental questions: what needs to be faithfully simulated in order to produce audiovisual virtual dynamics, and how to perform the simulation at minimal computational cost? The PI seeks to develop principled new techniques for complex nonlinear audiovisual simulations, which will lay the groundwork for building immersive simulated environments and interactive applications, and also for connecting to other areas such as computational design. At a technical level, the PI's approach will bridge the longstanding gap between theoretical tools (including multi-scale analysis and dynamical system analysis) and practical computational schemes. The PI argues that by decomposing the simulated dynamics into visible and audible scales, efficient simulation models adapted to individual scales can be built. This scheme can yield methods with high efficiency while retaining simulation fidelity. It also poses a new challenge: to develop adaptive simulation models across a wide range of audiovisual scales. The PI's approach will address this challenge by exploiting dynamical system analysis at individual scales and linking the resulting models together. If successful, this research will enable new simulated environment applications with fully synchronized audiovisual effects, and lead to new ways of creating and editing multimedia content.</AbstractNarration>
    <MinAmdLetterDate>02/02/2015</MinAmdLetterDate>
    <MaxAmdLetterDate>04/06/2015</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1453101</AwardID>
    <Investigator>
      <FirstName>Changxi</FirstName>
      <LastName>Zheng</LastName>
      <EmailAddress>cz2280@columbia.edu</EmailAddress>
      <StartDate>02/02/2015</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Columbia University</Name>
      <CityName>NEW YORK</CityName>
      <ZipCode>100276902</ZipCode>
      <PhoneNumber>2128546851</PhoneNumber>
      <StreetAddress>2960 Broadway</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>New York</StateName>
      <StateCode>NY</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7367</Code>
      <Text>Cyber-Human Systems (CHS)</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>1045</Code>
      <Text>CAREER: FACULTY EARLY CAR DEV</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7367</Code>
      <Text>Cyber-Human Systems</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9251</Code>
      <Text>RES EXPER FOR UNDERGRAD-SUPPLT</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7453</Code>
      <Text>GRAPHICS &amp; VISUALIZATION</Text>
    </ProgramReference>
  </Award>
</rootTag>
