<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>NRI: Collaborative Research: Shall I Touch This?: Navigating the Look and Feel of Complex Surfaces</AwardTitle>
    <AwardEffectiveDate>07/15/2014</AwardEffectiveDate>
    <AwardExpirationDate>06/30/2017</AwardExpirationDate>
    <AwardAmount>600000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Directorate for Computer &amp; Information Science &amp; Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Jie Yang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This project improves autonomous robotic perception so that future co-robots can glance around any scene and accurately estimate how it would feel to grasp or step on all of the visible surfaces. Just as people do, robots should use such these physical predictions to guide their interactions with the world, for example avoiding dangerous ice patches on the ground when walking and driving, and adeptly anticipating the grasp force needed to pick up everything from ice cubes to stuffed animals. These research activities are accompanied by significant outreach efforts, including a new program on "Look and Touch Robotics" to get middle-school students, particularly those from under­represented groups, excited about computer science, engineering, and robotics. This program uses simple experiments to highlight the dual importance of visual and haptic information during interactions with physical objects, along with demonstrations of a robot showing visuo-­haptic intelligence. This project also integrates research and education by involving undergraduates in the research and via hands-on projects in the vision and robotics classes taught by the Principal Investigators.&lt;br/&gt;&lt;br/&gt;This research involves extensive collection of data from real objects and surfaces using both visual and haptic sensors. The recorded interactions are analyzed to uncover visual clues that can allow a robot to infer the physical characteristics of the surface, such as slipperiness, hardness, and roughness. This problem is addressed using deep learning, a recently developed approach that has been successful in enabling robots to visually recognize a wide variety of objects in diverse circumstances. The research team also builds the database of visuo-haptic recordings and the learned cross-modal sensory, and makes it available to other robotics researchers at the end of the project.</AbstractNarration>
    <MinAmdLetterDate>08/18/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>08/18/2014</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1427425</AwardID>
    <Investigator>
      <FirstName>Trevor</FirstName>
      <LastName>Darrell</LastName>
      <EmailAddress>trevor@eecs.berkeley.edu</EmailAddress>
      <StartDate>08/18/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of California-Berkeley</Name>
      <CityName>BERKELEY</CityName>
      <ZipCode>947045940</ZipCode>
      <PhoneNumber>5106428109</PhoneNumber>
      <StreetAddress>Sponsored Projects Office</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
  </Award>
</rootTag>
