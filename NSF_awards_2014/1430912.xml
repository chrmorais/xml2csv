<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>SBIR Phase II: A Cloud-Based Service for Audio Access to News and Blogs</AwardTitle>
    <AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
    <AwardExpirationDate>09/30/2016</AwardExpirationDate>
    <AwardAmount>729756</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>07070000</Code>
      <Directorate>
        <LongName>Directorate for Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Industrial Innovation and Partnerships</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Glenn H. Larsen</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This SBIR Phase II project will research the algorithms for automatic discovery of topics of interest for a user based on existing written content such as news and blogs. These topics can then be used to dynamically create a personalized content reader with high quality synthesized audio. Applications created from this cloud-based project will enable a car driver to get instant and relevant information without taking her eyes off the road, a must for today's lengthy commutes and in-car safety. Other applications will allow the same audio access to Internet news and blogs via a smartphone for those who are vision-impaired or vision-busy with exercising, gardening, etc. This project has a societal impact among the blind, aging eyes that have difficulty reading small print and small screens, and the car driver, since it provides the ability to find contextual news and blogs in an "eyes-free" manner and with an easy listening experience. The fundamental research components from this project can be re-applied to other content and similar fields of research. This project's applications have the potential to generate a revenue stream which will in turn create jobs and have an overall impact on the economy.&lt;br/&gt;&lt;br/&gt;The goal of the research is to determine whether topic information extracted from a large corpus of unrelated documents using unsupervised machine-learning can be used for content discovery, improving the quality of synthesized speech, and discovering user preferences for a recommendation system. The research uses topic-modeling, a machine learning algorithm, to uncover topics across thousands of RSS feeds, and natural language processing to improve the quality of synthesized speech. Retrieval of specific RSS channels per user's topic preferences is then possible by using the probability of mappings between topics and RSS channels. Since content scanning by listening is a slower process than visually scanning for relevant responses, the current research proposal will improve this process by combining user preference with information retrieval for a better user experience. The topic discovery research will include three key components: discovering multiple levels of subtopics to create topic hierarchies for easier browsing, a method for identifying trending topics, and determining current and relevant topics for automation of audio content. Once this project is shown to produce effective results, the same techniques can be applied across other document collections.</AbstractNarration>
    <MinAmdLetterDate>09/05/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>09/05/2014</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1430912</AwardID>
    <Investigator>
      <FirstName>Radhika</FirstName>
      <LastName>Thekkath</LastName>
      <EmailAddress>rthekkath@agivox.com</EmailAddress>
      <StartDate>09/05/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>AgiVox, Inc.</Name>
      <CityName>Sunnyvale</CityName>
      <ZipCode>940853869</ZipCode>
      <PhoneNumber>6509960224</PhoneNumber>
      <StreetAddress>440 N. Wolfe Road</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
  </Award>
</rootTag>
